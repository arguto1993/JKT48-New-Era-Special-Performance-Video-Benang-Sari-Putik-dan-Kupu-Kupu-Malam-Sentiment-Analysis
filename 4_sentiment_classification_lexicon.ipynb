{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from tensorflow.keras.layers import Bidirectional, Dense, Dropout, Embedding, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11650 entries, 0 to 11649\n",
      "Data columns (total 10 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   ID                   11650 non-null  object\n",
      " 1   Username             11650 non-null  object\n",
      " 2   Comment              11650 non-null  object\n",
      " 3   LikeCount            11650 non-null  int64 \n",
      " 4   ReplyCount           11650 non-null  int64 \n",
      " 5   Date                 11650 non-null  object\n",
      " 6   Comment_clean        11650 non-null  object\n",
      " 7   Comment_clean_words  11504 non-null  object\n",
      " 8   Sentiment_score      11650 non-null  int64 \n",
      " 9   Sentiment            11650 non-null  object\n",
      "dtypes: int64(3), object(7)\n",
      "memory usage: 910.3+ KB\n"
     ]
    }
   ],
   "source": [
    "dir_ = \"dataset/\"\n",
    "file_input = dir_ + \"oshibe_spv_comments_2025-01-15_labeled_lexicon.csv\"\n",
    "data = pd.read_csv(file_input)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hasil stem dari tahap sebelumnya mengandung missing values pada 'Comment_stem', karena kolom ini akan menjadi X maka row dengan missing values disini akan dihapus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 11504 entries, 0 to 11649\n",
      "Data columns (total 10 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   ID                   11504 non-null  object\n",
      " 1   Username             11504 non-null  object\n",
      " 2   Comment              11504 non-null  object\n",
      " 3   LikeCount            11504 non-null  int64 \n",
      " 4   ReplyCount           11504 non-null  int64 \n",
      " 5   Date                 11504 non-null  object\n",
      " 6   Comment_clean        11504 non-null  object\n",
      " 7   Comment_clean_words  11504 non-null  object\n",
      " 8   Sentiment_score      11504 non-null  int64 \n",
      " 9   Sentiment            11504 non-null  object\n",
      "dtypes: int64(3), object(7)\n",
      "memory usage: 988.6+ KB\n"
     ]
    }
   ],
   "source": [
    "data = data[pd.notnull(data['Comment_clean_words']) & (data['Comment_clean_words'] != '')].copy()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment_clean_words</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>teman-teman lagu lgbt gadis muda yang beranjak...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>performance videonya kaya memberitahu dampak b...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>persatu member kesempatan menunjukan potensiny...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fiks kedepan jkt48 release single mvnya kostum...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>malam rahasia bilang siapasiapa rahasia ah cah...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>terlepas kontroversi sejujurnya lagu represent...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>terlepas hate comen jujur kemajuan banget jkt4...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gila konsep mv keren banget good job jkt48</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>congrats jkt48 new era mini albumnya jkt48 jay...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>buay yang bilang lesbi salah tuh makna lumayan...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Comment_clean_words Sentiment\n",
       "0  teman-teman lagu lgbt gadis muda yang beranjak...  negative\n",
       "1  performance videonya kaya memberitahu dampak b...  positive\n",
       "2  persatu member kesempatan menunjukan potensiny...  positive\n",
       "3  fiks kedepan jkt48 release single mvnya kostum...  positive\n",
       "4  malam rahasia bilang siapasiapa rahasia ah cah...  negative\n",
       "5  terlepas kontroversi sejujurnya lagu represent...  positive\n",
       "6  terlepas hate comen jujur kemajuan banget jkt4...  positive\n",
       "7         gila konsep mv keren banget good job jkt48  positive\n",
       "8  congrats jkt48 new era mini albumnya jkt48 jay...  positive\n",
       "9  buay yang bilang lesbi salah tuh makna lumayan...  positive"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['Comment_clean_words', 'Sentiment']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define X & y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['Comment_clean_words']\n",
    "y = data['Sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM with TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_svm, X_test_svm, y_train_svm, y_test_svm = train_test_split(X_tfidf, y, test_size=0.2, random_state=48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM with TF-IDF Train Accuracy: 96.82%\n",
      "SVM with TF-IDF Test Accuracy: 91.09%\n",
      "\n",
      "SVM with TF-IDF Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.68      0.79       241\n",
      "     neutral       0.88      0.96      0.92      1195\n",
      "    positive       0.95      0.91      0.93       865\n",
      "\n",
      "    accuracy                           0.91      2301\n",
      "   macro avg       0.92      0.85      0.88      2301\n",
      "weighted avg       0.91      0.91      0.91      2301\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train SVM model\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train_svm, y_train_svm)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_svm_train = svm_model.predict(X_train_svm)\n",
    "y_pred_svm_test = svm_model.predict(X_test_svm)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_svm_train = accuracy_score(y_train_svm, y_pred_svm_train)\n",
    "accuracy_svm_test = accuracy_score(y_test_svm, y_pred_svm_test)\n",
    "\n",
    "print(f\"SVM with TF-IDF Train Accuracy: {accuracy_svm_train*100:.2f}%\")\n",
    "print(f\"SVM with TF-IDF Test Accuracy: {accuracy_svm_test*100:.2f}%\")\n",
    "print()\n",
    "print(\"SVM with TF-IDF Classification Report:\")\n",
    "print(classification_report(y_test_svm, y_pred_svm_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest with BoW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Bag of Words (BoW) representation\n",
    "vectorizer = CountVectorizer(max_features=5000)\n",
    "X_bow = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(X_bow, y, test_size=0.3, random_state=48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with Bag of Words Train Accuracy: 99.88%\n",
      "Random Forest with Bag of Words Test Accuracy: 90.09%\n",
      "\n",
      "Random Forest with Bag of Words Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.70      0.77       370\n",
      "     neutral       0.90      0.92      0.91      1737\n",
      "    positive       0.91      0.93      0.92      1345\n",
      "\n",
      "    accuracy                           0.90      3452\n",
      "   macro avg       0.89      0.85      0.87      3452\n",
      "weighted avg       0.90      0.90      0.90      3452\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train Random Forest model\n",
    "rf_model_bow = RandomForestClassifier(n_estimators=48, random_state=48)\n",
    "rf_model_bow.fit(X_train_bow, y_train_bow)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_bow_train = rf_model_bow.predict(X_train_bow)\n",
    "y_pred_bow_test = rf_model_bow.predict(X_test_bow)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_bow_train = accuracy_score(y_train_bow, y_pred_bow_train)\n",
    "accuracy_bow_test = accuracy_score(y_test_bow, y_pred_bow_test)\n",
    "\n",
    "print(f\"Random Forest with Bag of Words Train Accuracy: {accuracy_bow_train * 100:.2f}%\")\n",
    "print(f\"Random Forest with Bag of Words Test Accuracy: {accuracy_bow_test * 100:.2f}%\")\n",
    "print()\n",
    "print(\"Random Forest with Bag of Words Classification Report:\")\n",
    "print(classification_report(y_test_bow, y_pred_bow_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['Comment_clean_words'].values\n",
    "y = data['Sentiment'].values\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment_clean_words</th>\n",
       "      <th>Word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9554</th>\n",
       "      <td>malam rahasia bilang siapasiapa rahasia ah cah...</td>\n",
       "      <td>406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>tari perut belly dance dikenal sebutan raqs sh...</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>performance videonya kaya memberitahu dampak b...</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>lyrics japanese naisho konya atta koto dare in...</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3508</th>\n",
       "      <td>resep niku udon ala marugame bahan g udon basa...</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2427</th>\n",
       "      <td>freya</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6633</th>\n",
       "      <td>ngeri</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6638</th>\n",
       "      <td>mantap</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6640</th>\n",
       "      <td>meresahkan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11649</th>\n",
       "      <td>ok</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11504 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Comment_clean_words  Word_count\n",
       "9554   malam rahasia bilang siapasiapa rahasia ah cah...         406\n",
       "256    tari perut belly dance dikenal sebutan raqs sh...         297\n",
       "1      performance videonya kaya memberitahu dampak b...         246\n",
       "729    lyrics japanese naisho konya atta koto dare in...         197\n",
       "3508   resep niku udon ala marugame bahan g udon basa...         185\n",
       "...                                                  ...         ...\n",
       "2427                                               freya           1\n",
       "6633                                               ngeri           1\n",
       "6638                                              mantap           1\n",
       "6640                                          meresahkan           1\n",
       "11649                                                 ok           1\n",
       "\n",
       "[11504 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Word_count'] = data['Comment_clean_words'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "data[['Comment_clean_words', 'Word_count']].sort_values('Word_count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 95th percentile of word count is: 20\n"
     ]
    }
   ],
   "source": [
    "# Calculate the 90th percentile of the word count\n",
    "percentile_95 = np.percentile(data['Word_count'], 95)\n",
    "\n",
    "print(f\"The 95th percentile of word count is: {percentile_95:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "max_words = 10000\n",
    "max_length = int(percentile_95)  # Maximum number of words in a comment\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "X_seq = tokenizer.texts_to_sequences(X)\n",
    "X_padded = pad_sequences(X_seq, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_encoded, test_size=0.2, random_state=48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 59ms/step - accuracy: 0.6699 - loss: 0.7538 - val_accuracy: 0.9066 - val_loss: 0.2831\n",
      "Epoch 2/5\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 56ms/step - accuracy: 0.9294 - loss: 0.1967 - val_accuracy: 0.9161 - val_loss: 0.2419\n",
      "Epoch 3/5\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 52ms/step - accuracy: 0.9758 - loss: 0.0746 - val_accuracy: 0.9309 - val_loss: 0.2294\n",
      "Epoch 4/5\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 45ms/step - accuracy: 0.9900 - loss: 0.0377 - val_accuracy: 0.9257 - val_loss: 0.2648\n",
      "Epoch 5/5\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 46ms/step - accuracy: 0.9934 - loss: 0.0233 - val_accuracy: 0.9274 - val_loss: 0.2995\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_words, output_dim=128),\n",
    "    Bidirectional(LSTM(128, return_sequences=True)),\n",
    "    Dropout(0.2),\n",
    "    Bidirectional(LSTM(64)),\n",
    "    Dropout(0.2),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m288/288\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "Deep Learning Train Accuracy: 99.43%\n",
      "Deep Learning Test Accuracy: 92.74%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.98      0.67      0.80       241\n",
      "     neutral       0.90      0.97      0.93      1195\n",
      "    positive       0.96      0.94      0.95       865\n",
      "\n",
      "    accuracy                           0.93      2301\n",
      "   macro avg       0.95      0.86      0.89      2301\n",
      "weighted avg       0.93      0.93      0.93      2301\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred_test, axis=1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_dl_train = model.evaluate(X_train, y_train, verbose=0)[1]\n",
    "accuracy_dl_test = model.evaluate(X_test, y_test, verbose=0)[1]\n",
    "print(f\"Deep Learning Train Accuracy: {accuracy_dl_train * 100:.2f}%\")\n",
    "print(f\"Deep Learning Test Accuracy: {accuracy_dl_test * 100:.2f}%\")\n",
    "print()\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_test, y_pred_classes, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Test with Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset for Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22996 entries, 0 to 22995\n",
      "Data columns (total 8 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   ID          22996 non-null  object \n",
      " 1   ParentID    11079 non-null  object \n",
      " 2   Timestamp   22996 non-null  object \n",
      " 3   Username    22996 non-null  object \n",
      " 4   Comment     22993 non-null  object \n",
      " 5   LikeCount   22996 non-null  int64  \n",
      " 6   ReplyCount  11917 non-null  float64\n",
      " 7   Date        22996 non-null  object \n",
      "dtypes: float64(1), int64(1), object(6)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "file_input_replies = dir_ + \"oshibe_spv_comments_2025-01-15.csv\"\n",
    "comment_replies = pd.read_csv(file_input_replies)\n",
    "comment_replies.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ParentID</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22947</th>\n",
       "      <td>UgxKVTACvziGZr5oR2R4AaABAg</td>\n",
       "      <td>@@anabell9316 ada mini album khusus lagu2 yang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22948</th>\n",
       "      <td>UgxKVTACvziGZr5oR2R4AaABAg</td>\n",
       "      <td>‚Äã@anra deka dipilih secara manajemen, yg mana ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22949</th>\n",
       "      <td>UgxKVTACvziGZr5oR2R4AaABAg</td>\n",
       "      <td>@@humptydumpty8943 owalah beda ya? Jujur nungg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22950</th>\n",
       "      <td>UgxKVTACvziGZr5oR2R4AaABAg</td>\n",
       "      <td>@@humptydumpty8943 only today dan musim panas ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22951</th>\n",
       "      <td>UgxKVTACvziGZr5oR2R4AaABAg</td>\n",
       "      <td>@@welliambadri29 harusnya semua performance vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22952</th>\n",
       "      <td>UgxKVTACvziGZr5oR2R4AaABAg</td>\n",
       "      <td>@@humptydumpty8943 mantap.. tapi sayangnya Jin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22953</th>\n",
       "      <td>UgxKVTACvziGZr5oR2R4AaABAg</td>\n",
       "      <td>Emng ya? Ya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22954</th>\n",
       "      <td>UgxKVTACvziGZr5oR2R4AaABAg</td>\n",
       "      <td>Iya bener</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22955</th>\n",
       "      <td>UgxKVTACvziGZr5oR2R4AaABAg</td>\n",
       "      <td>‚Äã@@vanellope05 ssk tuh apa?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22956</th>\n",
       "      <td>UgxKVTACvziGZr5oR2R4AaABAg</td>\n",
       "      <td>@@welliambadri29 BANG UDAH BANG STOP BANG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22957</th>\n",
       "      <td>UgxKVTACvziGZr5oR2R4AaABAg</td>\n",
       "      <td>‚Äã‚Äã@@welliambadri29esok kak mini albumnya diril...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22958</th>\n",
       "      <td>UgxKVTACvziGZr5oR2R4AaABAg</td>\n",
       "      <td>Aamiin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22963</th>\n",
       "      <td>UgxCXJhXOWtorqLHqCZ4AaABAg</td>\n",
       "      <td>New song dalam keterangannya di Twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22970</th>\n",
       "      <td>UgxEJNHgiICwOJyUvN54AaABAg</td>\n",
       "      <td>Amiin ya allah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22972</th>\n",
       "      <td>UgxRImDNvvZLHAQpMdN4AaABAg</td>\n",
       "      <td>Tebakan yang sangat akurat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22978</th>\n",
       "      <td>UgwbfBy7tSP4XWpmQjx4AaABAg</td>\n",
       "      <td>Tolong hapus emot biar trending</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22979</th>\n",
       "      <td>UgwbfBy7tSP4XWpmQjx4AaABAg</td>\n",
       "      <td>@@AbdulSalam-xe2cq kenapa woy üòëüòë</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22983</th>\n",
       "      <td>Ugwxd5VGdiMxfkC4Ck14AaABAg</td>\n",
       "      <td>Dh lewat ngav kwkw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22990</th>\n",
       "      <td>UgxO7uT6vEj69cqUJNd4AaABAg</td>\n",
       "      <td>Kita sama banget</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22992</th>\n",
       "      <td>Ugw36_aawjRCZxYm5Jl4AaABAg</td>\n",
       "      <td>Tolong hapus emot biar trending</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         ParentID  \\\n",
       "22947  UgxKVTACvziGZr5oR2R4AaABAg   \n",
       "22948  UgxKVTACvziGZr5oR2R4AaABAg   \n",
       "22949  UgxKVTACvziGZr5oR2R4AaABAg   \n",
       "22950  UgxKVTACvziGZr5oR2R4AaABAg   \n",
       "22951  UgxKVTACvziGZr5oR2R4AaABAg   \n",
       "22952  UgxKVTACvziGZr5oR2R4AaABAg   \n",
       "22953  UgxKVTACvziGZr5oR2R4AaABAg   \n",
       "22954  UgxKVTACvziGZr5oR2R4AaABAg   \n",
       "22955  UgxKVTACvziGZr5oR2R4AaABAg   \n",
       "22956  UgxKVTACvziGZr5oR2R4AaABAg   \n",
       "22957  UgxKVTACvziGZr5oR2R4AaABAg   \n",
       "22958  UgxKVTACvziGZr5oR2R4AaABAg   \n",
       "22963  UgxCXJhXOWtorqLHqCZ4AaABAg   \n",
       "22970  UgxEJNHgiICwOJyUvN54AaABAg   \n",
       "22972  UgxRImDNvvZLHAQpMdN4AaABAg   \n",
       "22978  UgwbfBy7tSP4XWpmQjx4AaABAg   \n",
       "22979  UgwbfBy7tSP4XWpmQjx4AaABAg   \n",
       "22983  Ugwxd5VGdiMxfkC4Ck14AaABAg   \n",
       "22990  UgxO7uT6vEj69cqUJNd4AaABAg   \n",
       "22992  Ugw36_aawjRCZxYm5Jl4AaABAg   \n",
       "\n",
       "                                                 Comment  \n",
       "22947  @@anabell9316 ada mini album khusus lagu2 yang...  \n",
       "22948  ‚Äã@anra deka dipilih secara manajemen, yg mana ...  \n",
       "22949  @@humptydumpty8943 owalah beda ya? Jujur nungg...  \n",
       "22950  @@humptydumpty8943 only today dan musim panas ...  \n",
       "22951  @@welliambadri29 harusnya semua performance vi...  \n",
       "22952  @@humptydumpty8943 mantap.. tapi sayangnya Jin...  \n",
       "22953                                        Emng ya? Ya  \n",
       "22954                                          Iya bener  \n",
       "22955                        ‚Äã@@vanellope05 ssk tuh apa?  \n",
       "22956          @@welliambadri29 BANG UDAH BANG STOP BANG  \n",
       "22957  ‚Äã‚Äã@@welliambadri29esok kak mini albumnya diril...  \n",
       "22958                                             Aamiin  \n",
       "22963            New song dalam keterangannya di Twitter  \n",
       "22970                                     Amiin ya allah  \n",
       "22972                         Tebakan yang sangat akurat  \n",
       "22978                    Tolong hapus emot biar trending  \n",
       "22979                   @@AbdulSalam-xe2cq kenapa woy üòëüòë  \n",
       "22983                                 Dh lewat ngav kwkw  \n",
       "22990                                   Kita sama banget  \n",
       "22992                    Tolong hapus emot biar trending  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "replies = comment_replies[comment_replies['ParentID'].notnull()].copy()\n",
    "display(replies[['ParentID', 'Comment']].tail(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ini bukan lgbt, ini menceritakan tentang salah pergaulan'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replies.loc[11, 'Comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mohon maaf kak, anda sedang menghadapi masalah apa sehingga terpikirkan untuk berkomentar demikian?'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replies.loc[26, 'Comment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Test with some Comment Replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Reply: Ini bukan lgbt, ini menceritakan tentang salah pergaulan\n",
      "Predicted Sentiment: neutral\n",
      "------------------------------\n",
      "Reply: Jelas sekali bapak nya ga ngerti lagunya artinya apa..dikit2 lgbt kalo sama cowok nanti dikata haram dasar tua\n",
      "Predicted Sentiment: neutral\n",
      "------------------------------\n",
      "Reply: Mohon maaf kak, anda sedang menghadapi masalah apa sehingga terpikirkan untuk berkomentar demikian?\n",
      "Predicted Sentiment: positive\n",
      "------------------------------\n",
      "Reply: ih apa sih ini kok jkt48 jadi lesbi?\n",
      "Predicted Sentiment: neutral\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Some replies for inference\n",
    "sample_replies = [\n",
    "    \"Ini bukan lgbt, ini menceritakan tentang salah pergaulan\",\n",
    "    \"Jelas sekali bapak nya ga ngerti lagunya artinya apa..dikit2 lgbt kalo sama cowok nanti dikata haram dasar tua\",\n",
    "    \"Mohon maaf kak, anda sedang menghadapi masalah apa sehingga terpikirkan untuk berkomentar demikian?\",\n",
    "    \"ih apa sih ini kok jkt48 jadi lesbi?\"\n",
    "]\n",
    "\n",
    "# Preprocess the new comments (tokenize and pad)\n",
    "new_sequences = tokenizer.texts_to_sequences(sample_replies)\n",
    "new_padded = pad_sequences(new_sequences, maxlen=max_length)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(new_padded)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Decode the predicted classes into sentiment labels\n",
    "predicted_labels = label_encoder.inverse_transform(predicted_classes)\n",
    "\n",
    "# Output results\n",
    "for comment, label in zip(sample_replies, predicted_labels):\n",
    "    print(f\"Reply: {comment}\")\n",
    "    print(f\"Predicted Sentiment: {label}\")\n",
    "    print(\"-\" * 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
